# ETA Pipeline Developer's Guide

This document describes how to add new analytics pipelines to ETA. See
`modules_dev_guide.md` for more information about adding new modules to ETA,
and see `core_dev_guide.md` for instructions on contributing to the core ETA
infrastructure.


## What are ETA Pipelines?

Pipelines are the mechanisms by which analytics capabilities are exposed to the
users of the ETA system. Pipelines define the pre-packaged computations that
users can run on their data, and they expose the relevant inputs, outputs, and
tunable parameters of the analytic that the user can provide, access, and
customize.

Each ETA pipeline is represented internally as a graph whose nodes are ETA
modules and whose edges define the flow of data between the modules. Thus the
ETA pipeline system is general purpose and highly customizable. The ETA
repository defines a collection of pre-configured pipelines that combine the
builtin ETA modules in many ways to useful video analytics capabilities.\

New pipelines can be easily added to the ETA system by writing a simple JSON
configuration file whose syntax is described in the next section.


## Pipeline Metadata JSON Files

Every ETA pipeline must provide a metadata JSON file describing the inputs,
outputs, modules (nodes), and data flow (edges) of the computational graph.

The metadata file contains all the necessary information to instantiate the
pipeline and module configuration files that are required under-the-hood to run
an ETA pipeline on data. The pipeline and associated metadata files of the
constituent modules define a template that is populated by the pipeline builder
for each new piece of input data.

The following JSON gives an example of the metadata file for a simple object
detection pipeline:

```json
{
    "info": {
        "name": "Simple-Object-Detector",
        "version": "0.1.0",
        "description": "A simple object detection pipeline",
        "id": "p1pszsdbmiw4ean4"
    },
    "modules": [
        {
            "label": "resize",
            "module": "eta/modules/resize_videos"
        },
        {
            "label": "sample",
            "module": "eta/modules/sample_videos"
        },
        {
            "label": "detect",
            "module": "eta/modules/detect_objects"
        },
        {
            "label": "visualize",
            "module": "eta/modules/visualize_objects"
        },
    ],
    "connections": [
        {
            "source": "INPUT1",
            "sink": "resize.input_path"
        },
        {
            "source": "INPUT1",
            "sink": "visualize.raw_video_path"
        },
        {
            "source": "resize.outpath_path",
            "sink": "sample.input_path"
        },
        {
            "source": "sample.outpath_path",
            "sink": "detect.raw_video_path"
        },
        {
            "source": "detect.objects_json_path",
            "sink": "visualize.objects_json_path"
        },
        {
            "source": "detect.objects_json_path",
            "sink": "OUTPUT1"
        },
        {
            "source": "visualize.visual_vidpath",
            "sink": "OUTPUT2"
        }
    ]
}
```

When discussing pipeline metadata files, we refer to each JSON object `{}` as a
**spec** (specification) because it specifies the semantics of a certain
entity, and we refer to the keys of a JSON object (e.g., "info") as **fields**.

The pipeline metadata file contains the following top-level fields:

- `info`: a spec containing basic information about the module

- `modules`: a list of specs describing the modules (nodes) in the pipeline

- `connections`: a list of specs describing the connections (edges) between
    modules in the pipeline

The `info` spec contains the following fields:

- `name`: the name of the pipeline

- `version`: the current pipeline version

- `description`: a short free-text description of the pipeline purpose and
    implementation

- `id`: the ID of the pipeline, which is a unique identifier generated by the
    ETA maintainers when a new pipeline is registered with the ETA system

The `modules` field contains a list of module (node) specs with the following
fields:

- `label`: a label for the module for use when defining connections

- `module`: a pointer to the module implementation

The `connections` field contains a list of connection (edge) specs with the
following fields:

- `source`: the source (starting point) of the edge. The syntax for a source is
    `"<module_label>.<output_field_name>"`. Alternatively, the special values
    `"INPUT1"`, `"INPUT2"`, etc. can be used to designate a module input as a
    pipeline input

- `sink`: the sink (stopping point) of the edge. The syntax for a sink is
    `"<module_label>.<input_field_name>"`. Alternatively, the special values
    `"OUTPUT1"`, `"OUTPUT2"`, etc. can be used to designate a module output as
    a pipeline output

The pipeline metadata file defines the connectivity of the computation graph.
In practice, the pipeline builder introspects


## Visualizing Pipeline Graphs

The ETA system provides the ability to visualize pipelines as block diagram
images using the [blockdiag](https://pypi.python.org/pypi/blockdiag) package.

#### Generating a block diagram file

The first step is to generate a block diagram file for the pipeline of
interest. The ETA system provides a convenient `to_blockdiag` method to
generate the `.diag` file.

For example, the block diagram file for the simple object detector described
above can be generated by calling:

```python
from eta.core.pipeline import PipelineMetadataConfig

# Generate block diagram file
json_path = "eta/pipelines/simple_object_detector.json"
blockdiag_path = "eta/pipelines/simple_object_detector.diag"
PipelineMetadataConfig.from_json(json_path).to_blockdiag(blockdiag_path)
```

The resulting `.diag` file looks like this:

```
blockdiag {
  // modules
  resize [shape = box];
  sample [shape = box];
  detect [shape = box];
  visualize [shape = box];

  // inputs
  INPUT1 [shape = endpoint];

  // outputs
  OUTPUT1 [shape = endpoint];
  OUTPUT2 [shape = endpoint];

  // parameters
  size [shape = beginpoint];
  fps [shape = beginpoint];
  config_dir [shape = beginpoint];
  model_path [shape = beginpoint];
  weights_path [shape = beginpoint];
  use_gpu [shape = beginpoint];
  labels1 [shape = beginpoint];
  threshold1 [shape = beginpoint];
  show_confidence [shape = beginpoint];
  labels2 [shape = beginpoint];
  threshold2 [shape = beginpoint];
  color [shape = beginpoint];

  // I/O connections
  INPUT1 -> resize;
  INPUT1 -> visualize;
  resize -> sample;
  sample -> detect;
  detect -> visualize;
  detect -> OUTPUT1;
  visualize -> OUTPUT2;

  // parameter connections
  group {
    orientation = portrait
    size -> resize;
  }

  group {
    orientation = portrait
    fps -> sample;
  }

  group {
    orientation = portrait
    config_dir -> detect;
    model_path -> detect;
    weights_path -> detect;
    use_gpu -> detect;
    labels1 -> detect;
    threshold1 -> detect;
  }

  group {
    orientation = portrait
    show_confidence -> visualize;
    labels2 -> visualize;
    threshold2 -> visualize;
    color -> visualize;
  }
}
```

#### Visualizing a block diagram file

To visualize a pipeline as a block diagram image, simply use the `blockdiag`
tool to render the `.diag` file. For example:

```shell
# Render block diagram as SVG
blockdiag -Tsvg eta/pipelines/simple_object_detector.diag

# View block diagram
open eta/pipelines/simple_object_detector.svg
```
